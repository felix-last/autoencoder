{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# DATA\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.cluster\n",
    "import sklearn.model_selection\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import random\n",
    "#PLOTTING\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "# MISC\n",
    "import os\n",
    "import traceback\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import yaml\n",
    "# UTIL\n",
    "import util.intracluster_smote as intracluster_smote\n",
    "import util.evaluation as evaluation\n",
    "# DATASET UTIL\n",
    "import util.dataset.mnist_utils as mnist_utils\n",
    "import util.dataset.asirra_utils as asirra_utils\n",
    "# AUTOENCODER\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import advanced_activations\n",
    "# Fixup for keras for Tensorboard > 0.12\n",
    "import tensorflow as tf\n",
    "tf.merge_all_summaries = tf.summary.merge_all\n",
    "tf.train.SummaryWriter = tf.summary.FileWriter\n",
    "import util.autoencoder_keras as autoencoder_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate a session_id as a folder name to save run results\n",
    "if 'session_id' not in globals():\n",
    "    session_id = (datetime.utcnow() + timedelta(hours=2,minutes=0)).strftime(\"%Y-%m-%d %Hh%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exeriment Parameters\n",
    "experiment_params = {\n",
    "    'session_id': session_id,\n",
    "    'random_seed': int(os.urandom(1)[0] / 255 * (2**32)), # unsigned 32-bit int between 0 and 2**32\n",
    "    'SMOTE_k': 5,\n",
    "    'Creditcard Fraud': {\n",
    "        'k':1000,\n",
    "        # KMEANS\n",
    "        'kmeans_n_init':3,\n",
    "        # AE\n",
    "        'ae_layers': [30,2,30],\n",
    "        'ae_n_epoch': 100,\n",
    "        'ae_n_init':50,\n",
    "        'ae_optimizer': 'adagrad',\n",
    "        'ae_batch_size': 1000,\n",
    "        'ae_default_activation_fn': advanced_activations.LeakyReLU,\n",
    "        'ae_default_activation_fn_params': {'alpha': 0.01}\n",
    "    }\n",
    "}\n",
    "np.random.seed(experiment_params['random_seed'])\n",
    "random.seed(experiment_params['random_seed'])\n",
    "\n",
    "# generate a session folder\n",
    "if session_id not in os.listdir('results'):\n",
    "    os.mkdir('results/{0}'.format(session_id))\n",
    "\n",
    "# save experiment parameters\n",
    "with open('results/{0}/experiment_parameters.yml'.format(session_id), 'w') as outfile:\n",
    "    yaml.dump(experiment_params, outfile, default_flow_style=False, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Algorithms\n",
    "These algorithms are used to cluster the dataset. Their output will later serve as input to Intracluster SMOTE to generate artificial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kmeans(X,k,X_test):\n",
    "#     if X.shape[0] < 5000: # 10000 recommended\n",
    "#         return sklearn.cluster.KMeans(n_clusters=k).fit(X).labels_\n",
    "#     else:\n",
    "    try:\n",
    "        return (\n",
    "            X, \n",
    "            sklearn.cluster.MiniBatchKMeans(n_clusters=k, init_size=min(2*k, X.shape[0]), n_init=experiment_params[dataset_name]['kmeans_n_init']).fit(X).labels_,\n",
    "            None, \n",
    "            {}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        np.save('results/{0}/{1}/{2}/error_data'.format(session_id, dataset_name,fold),X)\n",
    "        raise e\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define Clustering on Autoencoder Encoding\n",
    "def autoencoder_kmeans(X,k,X_test):\n",
    "    dim = X.shape[1]\n",
    "    ae = autoencoder_keras.Autoencoder(\n",
    "#         layers=[X.shape[1],128,64,min(32,X.shape[1]),64,128,X.shape[1]],    \n",
    "        # reduce dimensions by increasing fractions, but always retain at least 5,4,3 features for low dimensional datasets\n",
    "        # asirra 100x100x3: 3000, 1500, 750, 375, ..., 3000\n",
    "        # mnist 28x28: 784, 392, 196, 98, ..., 784\n",
    "        # ecoli: 7, 6, 5, 5, ..., 7 # don't use anymore - dataset too small\n",
    "        # cc: 30, 15, 15, 10, ..., 30\n",
    "#         layers=[dim, max(dim//2,15), max(dim//4,15), max(dim//8,10), max(dim//4,15), max(dim//2,15),dim], \n",
    "        layers=experiment_params[dataset_name]['ae_layers'], # optimized for cc\n",
    "        optimizer=experiment_params[dataset_name]['ae_optimizer'],  # optimized for cc\n",
    "        training_set=X,\n",
    "        n_init = experiment_params[dataset_name]['ae_n_init'],\n",
    "        default_activation_fn = experiment_params[dataset_name]['ae_default_activation_fn'](**experiment_params[dataset_name]['ae_default_activation_fn_params'])\n",
    "    )\n",
    "    ae.fit(X, X,\n",
    "            nb_epoch=experiment_params[dataset_name]['ae_n_epoch'], # optimized for cc\n",
    "            batch_size=experiment_params[dataset_name]['ae_batch_size'], # optimized for cc\n",
    "            shuffle=True,\n",
    "            verbose=0,\n",
    "            callbacks=[TensorBoard(log_dir=('./results/{0}/{1}/{2}/{3}'.format(session_id,dataset_name,fold,'autoencoder_kmeans')))]\n",
    "    )\n",
    "    np.save(\n",
    "        'results/{0}/{1}/{2}/{3}/ae_weights'.format(session_id, dataset_name,fold,'autoencoder_kmeans'),\n",
    "        ae.get_parameters()\n",
    "    )\n",
    "    H = ae.encode(X)\n",
    "    _, clusters, _, additional_benchmark_fields = kmeans(H,k,X_test)\n",
    "    additional_benchmark_fields['AE Loss CE'] = ae.get_cost()\n",
    "    additional_benchmark_fields['AE Loss MSE'] = np.mean((X - ae.decode(ae.encode(X)))**2)\n",
    "    additional_benchmark_fields['AE Loss CE Validation'] = ae.get_cost(data=X_test)\n",
    "    additional_benchmark_fields['AE Loss MSE Validation'] = np.mean((X_test - ae.decode(ae.encode(X_test)))**2)\n",
    "    return (H, clusters, ae.decode, additional_benchmark_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Clustering on Autoencoder Encoding, returning unencoded X only\n",
    "def autoencoder_kmeans_unencoded(X,k,X_test):\n",
    "    dim = X.shape[1]\n",
    "    ae = autoencoder_keras.Autoencoder(\n",
    "        layers=experiment_params[dataset_name]['ae_layers'], # optimized for cc\n",
    "        optimizer=experiment_params[dataset_name]['ae_optimizer'],  # optimized for cc\n",
    "        training_set=X,\n",
    "        n_init = experiment_params[dataset_name]['ae_n_init'],\n",
    "        default_activation_fn = experiment_params[dataset_name]['ae_default_activation_fn'](**experiment_params[dataset_name]['ae_default_activation_fn_params'])\n",
    "    )\n",
    "    ae.fit(X, X,\n",
    "            nb_epoch=experiment_params[dataset_name]['ae_n_epoch'], # optimized for cc\n",
    "            batch_size=experiment_params[dataset_name]['ae_batch_size'], # optimized for cc\n",
    "            shuffle=True,\n",
    "            verbose=0,\n",
    "            callbacks=[TensorBoard(log_dir=('./results/{0}/{1}/{2}/{3}'.format(session_id,dataset_name,fold,'autoencoder_kmeans_decoded')))]\n",
    "    )\n",
    "    np.save(\n",
    "        'results/{0}/{1}/{2}/{3}/ae_weights'.format(session_id, dataset_name,fold,'autoencoder_kmeans_decoded'),\n",
    "        ae.get_parameters()\n",
    "    )\n",
    "    H = ae.encode(X)\n",
    "    _, clusters, _, additional_benchmark_fields = kmeans(H,k,X_test)\n",
    "    additional_benchmark_fields['AE Loss CE'] = ae.get_cost()\n",
    "    additional_benchmark_fields['AE Loss MSE'] = np.mean((X - ae.decode(ae.encode(X)))**2)\n",
    "    additional_benchmark_fields['AE Loss CE Validation'] = ae.get_cost(data=X_test)\n",
    "    additional_benchmark_fields['AE Loss MSE Validation'] = np.mean((X_test - ae.decode(ae.encode(X_test)))**2)\n",
    "    return (X, clusters, None, additional_benchmark_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define Autoencoder Clustering\n",
    "import util.autoencoder as autoencoder\n",
    "def autoencoder_clustering(X,k,X_test): \n",
    "    # Pretrain using Keras AE\n",
    "    ae = autoencoder_keras.Autoencoder(\n",
    "        layers=[X.shape[1],128,64,min(32,X.shape[1]),64,128,X.shape[1]], \n",
    "        training_set=X\n",
    "    )\n",
    "    ae.fit(X, X,\n",
    "        nb_epoch=10000,\n",
    "        batch_size=256,\n",
    "        shuffle=True,\n",
    "        verbose=0,\n",
    "        callbacks=[TensorBoard(log_dir=('./results/{0}/{1}/{2}'.format(session_id,dataset_name,fold)))]\n",
    "    )\n",
    "    np.save(\n",
    "        'results/{0}/{1}/{2}/ae_weights'.format(session_id, dataset_name,fold),\n",
    "        ae.get_parameters()\n",
    "    )\n",
    "    load_ae_weights_from_file = 'results/{0}/{1}/ae_parameters'.format(session_id,dataset_name),\n",
    "\n",
    "    # Cluster with clustering AE\n",
    "    clustering_ae = autoencoder.Autoencoder(\n",
    "        layers=[X.shape[1],128,64,min(32,X.shape[1]),64,128,X.shape[1]], \n",
    "        training_set=X\n",
    "    )\n",
    "    clustering_ae.set_parameters(ae.get_weights())\n",
    "    clustering_ae.cluster(epochs=50, eta=0.01, q=0.01, k=k, minibatch_size=256, mu=0.7)\n",
    "    H = clustering_ae.encode(X)\n",
    "    return (H, clustering_ae.get_cluster_assignment(X), clustering_ae.decode, {'ae_loss':ae.get_cost()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# each of these returns a tuple of (X_encoded, clustering_function(X,k), decoder, benchmark_fields)\n",
    "# only X and k are used by the algorithms; X_test must remain unseen!\n",
    "preprocessing_methods = {\n",
    "    'No Oversampling': (lambda X, k, X_test: (X, None, None, {})),\n",
    "    'SMOTE': (lambda X, k, X_test: (X, np.zeros(X.shape[0],), None, {})),\n",
    "#     'Random Clustering': (lambda X, k, X_test: (X, np.random.choice(k, size=(X.shape[0])), None)),\n",
    "    'K-Means': kmeans,\n",
    "    'K-Means on Autoencoder Encoding': autoencoder_kmeans,\n",
    "    'K-Means on Autoencoder Encoding (SMOTE Original Space)': autoencoder_kmeans_unencoded,\n",
    "#     'Autoencoder Clustering': autoencoder_clustering\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "Import some datasets to use to test the various data generation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Ecoli Dataset\n",
    "ecoli_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "def load_ecoli(ratio=None):\n",
    "    ecoli_data_raw = pd.read_csv('datasets/ecoli.data.txt', delim_whitespace=True, header=None, names=['Sequence Name','mcg','gvh','lip','chg','aac','alm1','alm2','class'])\n",
    "    ecoli_data_target = ecoli_data_raw['class'].as_matrix()\n",
    "    ecoli_data = ecoli_data_raw.drop(['Sequence Name','class'], axis=1).as_matrix()\n",
    "    ecoli_data = ecoli_scaler.fit_transform(ecoli_data)\n",
    "    return ecoli_data, ecoli_data_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Credit Card Dataset\n",
    "# https://www.kaggle.com/dalpozz/creditcardfraud\n",
    "cc_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "def load_cc(ratio=None):\n",
    "    cc_data_raw = pd.read_csv('datasets/creditcard.csv')\n",
    "    cc_data_target = cc_data_raw['Class']\n",
    "    cc_data = cc_data_raw.drop(['Class'], axis=1)\n",
    "    cc_data = cc_scaler.fit_transform(cc_data)\n",
    "    return np.asarray(cc_data), np.asarray(cc_data_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets = {\n",
    "#     'MNIST Imbalanced 7+8': {\n",
    "#         'DataRetriever': mnist_utils.load_binary_imbalanced_7_8,\n",
    "#         'Imbalance Ratio': 1/30,\n",
    "#         'Minority Class': 8,\n",
    "#         'Scaler': sklearn.preprocessing.MinMaxScaler().fit([[0],[255]]),\n",
    "#         'Plotter': mnist_utils.plot\n",
    "#     },\n",
    "#     'MNIST Imbalanced 1+7': {\n",
    "#         'DataRetriever': mnist_utils.load_binary_imbalanced_1_7,\n",
    "#         'Imbalance Ratio': 1/30,\n",
    "#         'Minority Class': 7,\n",
    "#         'Scaler': sklearn.preprocessing.MinMaxScaler().fit([[0],[255]]),\n",
    "#         'Plotter': mnist_utils.plot\n",
    "#     },\n",
    "#     'Asirra Imbalanced': {\n",
    "#         'DataRetriever': asirra_utils.load_data_imbalanced,\n",
    "#         'Imbalance Ratio': 1/30,\n",
    "#         'Minority Class': 1,\n",
    "#         'Scaler': sklearn.preprocessing.MinMaxScaler().fit([[0],[255]]),\n",
    "#         'Plotter': asirra_utils.plot\n",
    "#     },\n",
    "#     'Ecoli': {\n",
    "#         'DataRetriever': load_ecoli,\n",
    "#         'Imbalance Ratio': 1/5.46,\n",
    "#         'Minority Class':'pp',\n",
    "#         'Scaler': ecoli_scaler,\n",
    "#         'Plotter': None\n",
    "#     },\n",
    "    'Creditcard Fraud': {\n",
    "        'DataRetriever': load_cc,\n",
    "        'Imbalance Ratio': 1/577.87,\n",
    "        'Minority Class': 1,\n",
    "        'Scaler': cc_scaler,\n",
    "        'Plotter': None\n",
    "    }\n",
    "}\n",
    "def count(d):\n",
    "    return d['Data'][0].shape[0]\n",
    "def count_features(d):\n",
    "    return d['Data'][0].shape[1]\n",
    "def minorityCount(d): \n",
    "    return np.count_nonzero(d['Data'][1]==d['Minority Class'])\n",
    "def majorityCount(d):\n",
    "    return count(d) - minorityCount(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Classifiers\n",
    "These classifiers need no parameter configuration and lend themselves to benchmarking the effectiveness of data generation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'Logistic Regression': sklearn.linear_model.LogisticRegression(),\n",
    "#     'Random Forest': sklearn.ensemble.RandomForestClassifier(),\n",
    "#     'Gradient Boosting': sklearn.ensemble.GradientBoostingClassifier(),\n",
    "#     'Support Vector': sklearn.svm.SVC()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing it all together\n",
    "Loop through all the datasets and evaluate the performance of the different data balancing techniques using all above classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#####################################\n",
      "Dataset: Creditcard Fraud\n",
      "Dimensionality: 30 \n",
      "Observations: 284807 \n",
      "Minority Size: 492 \n",
      "Imbalance Ratio: 0.0017\n",
      "\n",
      "-------------------------------------\n",
      "Fold 1\n",
      "Preprocessing Method: No Oversampling\n",
      "Preprocessing Method: K-Means\n",
      "Preprocessing Method: K-Means on Autoencoder Encoding\n",
      "Choosing AE weights with cost 0.687061190298\n",
      "WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n",
      "Preprocessing Method: K-Means on Autoencoder Encoding (SMOTE Original Space)\n",
      "Choosing AE weights with cost 0.682569531037\n",
      "WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n",
      "Preprocessing Method: SMOTE\n",
      "\n",
      "-------------------------------------\n",
      "Fold 2\n",
      "Preprocessing Method: No Oversampling\n",
      "Preprocessing Method: K-Means\n",
      "Preprocessing Method: K-Means on Autoencoder Encoding\n",
      "Choosing AE weights with cost 0.679669923342\n",
      "WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n",
      "Preprocessing Method: K-Means on Autoencoder Encoding (SMOTE Original Space)\n",
      "Choosing AE weights with cost 0.675838161153\n",
      "WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n",
      "Preprocessing Method: SMOTE\n",
      "\n",
      "-------------------------------------\n",
      "Fold 3\n",
      "Preprocessing Method: No Oversampling\n",
      "Preprocessing Method: K-Means\n",
      "Preprocessing Method: K-Means on Autoencoder Encoding\n",
      "Choosing AE weights with cost 0.683546450172\n",
      "WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n",
      "Preprocessing Method: K-Means on Autoencoder Encoding (SMOTE Original Space)\n",
      "Choosing AE weights with cost 0.680611828426\n",
      "WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n",
      "Preprocessing Method: SMOTE\n",
      "\n",
      "-------------------------------------\n",
      "Fold 4\n",
      "Preprocessing Method: No Oversampling\n",
      "Preprocessing Method: K-Means\n",
      "Preprocessing Method: K-Means on Autoencoder Encoding\n",
      "Choosing AE weights with cost 0.685505441975\n",
      "WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n",
      "Preprocessing Method: K-Means on Autoencoder Encoding (SMOTE Original Space)\n",
      "Choosing AE weights with cost 0.678840325153\n",
      "WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n",
      "Preprocessing Method: SMOTE\n",
      "\n",
      "-------------------------------------\n",
      "Fold 5\n",
      "Preprocessing Method: No Oversampling\n",
      "Preprocessing Method: K-Means\n",
      "Preprocessing Method: K-Means on Autoencoder Encoding\n",
      "Choosing AE weights with cost 0.669685365088\n",
      "WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n",
      "Preprocessing Method: K-Means on Autoencoder Encoding (SMOTE Original Space)\n"
     ]
    }
   ],
   "source": [
    "classification_evaluation = {}\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    print('\\n#####################################')\n",
    "    print('Dataset:',dataset_name)\n",
    "    dataset['Data'] = dataset['DataRetriever'](ratio=dataset['Imbalance Ratio'])\n",
    "    print('Dimensionality:',count_features(dataset), '\\nObservations:', count(dataset), '\\nMinority Size:', minorityCount(dataset), '\\nImbalance Ratio:', round(dataset['Imbalance Ratio'],4))\n",
    "    os.mkdir('results/{0}/{1}'.format(session_id,dataset_name))\n",
    "    skf = sklearn.model_selection.StratifiedKFold(n_splits=5, random_state=experiment_params['random_seed'])\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(dataset['Data'][0],dataset['Data'][1])):\n",
    "        fold = fold+1\n",
    "        train, train_target = dataset['Data'][0][train_index], dataset['Data'][1][train_index]\n",
    "        test, test_target = dataset['Data'][0][test_index], dataset['Data'][1][test_index]\n",
    "        print('\\n-------------------------------------')\n",
    "        print('Fold',fold)#,'\\nObservations:',train.shape[0],'\\nMinority Size:',train[train_target==dataset['Minority Class']].shape[0])\n",
    "        try: os.mkdir('results/{0}/{1}/{2}'.format(session_id,dataset_name,fold))        \n",
    "        except: pass\n",
    "        for preprocessing_method_name, preprocessing_method in preprocessing_methods.items():        \n",
    "            print('Preprocessing Method:',preprocessing_method_name)\n",
    "\n",
    "            with warnings.catch_warnings():\n",
    "                if preprocessing_method_name.rfind('SMOTE') > -1:\n",
    "                    # for SMOTE, we dont cluster thus expect a warning. Ignore it.\n",
    "                    warnings.filterwarnings(action='ignore', category=UserWarning, module='util.intracluster_smote', message='No minority clusters')\n",
    "                else:\n",
    "                    # for clustering based algorithms, no minority clusters found makes the run invalid -> raise an exception\n",
    "                    warnings.filterwarnings(action='error', category=UserWarning, module='util.intracluster_smote', message='No minority clusters')\n",
    "            \n",
    "                try:\n",
    "                    k = experiment_params[dataset_name]['k'] #minorityCount(dataset) + ((majorityCount(dataset)-minorityCount(dataset))//100)\n",
    "                    if preprocessing_method_name.rfind('No Oversampling') > -1:\n",
    "                        (train_oversampled, train_target_oversampled) = train, train_target\n",
    "                        additional_benchmark_fields = {}\n",
    "                    else:\n",
    "                        #oversample\n",
    "                        train_encoded, cluster_labels, decoder, additional_benchmark_fields = preprocessing_method(train, k, test)\n",
    "                        required_synthetic_count =  intracluster_smote.compute_synthetic_count(train.shape[0], dataset['Imbalance Ratio'])\n",
    "                        save_creation_examples = 0.3#(30/count(dataset)) #if dataset['Plotter'] is not None else 0\n",
    "                        oversampler = intracluster_smote.IntraclusterSmote(required_synthetic_count, experiment_params['SMOTE_k'], decoder=decoder, save_creation_examples=save_creation_examples)\n",
    "                        (train_oversampled, train_target_oversampled), (synth_X, synth_y), oversampler_meta  = oversampler.fit_transform(train_encoded, train_target, dataset['Minority Class'], cluster_labels, train)\n",
    "                        additional_benchmark_fields['Diversity'] = evaluation.diversion_score(train, oversampler_meta['creation_examples'])\n",
    "                        additional_benchmark_fields['Minority Cluster Count'] = oversampler_meta['minority_cluster_count']\n",
    "                        # save the generated samples to file for later analysis\n",
    "                        np.save(\n",
    "                            'results/{0}/{1}/{2}/generated_samples_{3}'.format(session_id,dataset_name,fold,preprocessing_method_name.lower().replace(' ','_')), \n",
    "                            synth_X\n",
    "                        )\n",
    "                        if preprocessing_method_name.rfind('Autoencoder') > -1:\n",
    "                            np.save(\n",
    "                                'results/{0}/{1}/{2}/generated_samples_encoded_{3}'.format(session_id,dataset_name,fold,preprocessing_method_name.lower().replace(' ','_')),\n",
    "                                oversampler_meta['generated_samples_encoded']\n",
    "                            )\n",
    "                            if len(oversampler_meta['encoded_creation_examples']) > 0:\n",
    "                                np.save(\n",
    "                                    'results/{0}/{1}/{2}/encoded_creation_examples_{3}'.format(session_id,dataset_name,fold,preprocessing_method_name.lower().replace(' ','_')),\n",
    "                                    oversampler_meta['encoded_creation_examples']\n",
    "                                )\n",
    "                        if dataset['Plotter'] is not None:\n",
    "                            plot = dataset['Plotter']\n",
    "\n",
    "                            if preprocessing_method_name.rfind('Autoencoder') > -1:\n",
    "                                print('Reconstruction Quality Sample of', preprocessing_method_name)\n",
    "                                plot(train, train_target, 20)\n",
    "                                plot(decoder(train_encoded), train_target, 20)\n",
    "\n",
    "                            print('Plotting', dataset_name, 'synthetic samples after applying', preprocessing_method_name)\n",
    "                            plot(synth_X, synth_y, 20)\n",
    "\n",
    "                            print('Showing examples of inputs and resulting synthetic samples for', preprocessing_method_name, '(Parent A, Offspring, Parent B)')\n",
    "                            parent_a, offspring, parent_b = zip(*random.sample(oversampler_meta['creation_examples'], len(oversampler_meta['creation_examples']))) # shuffle before to make the plot more diverse\n",
    "                            for name, examples in [('A',parent_a), ('X', offspring), ('B', parent_b)]:\n",
    "                                examples = np.asarray(examples)\n",
    "                                plot(examples, [name for _ in examples], (20 if dataset_name.rfind('Asirra') < 0 else 10))\n",
    "                            # Remove unused data to let the garbage collector work\n",
    "                            del synth_X, synth_y, train_encoded, cluster_labels, decoder, oversampler, oversampler_meta\n",
    "\n",
    "                    for classifier_name, classifier in classifiers.items():\n",
    "                        # classify\n",
    "                        classifier.fit(train_oversampled,train_target_oversampled)\n",
    "                        prediction = classifier.predict(test)\n",
    "                        #evaluate\n",
    "                        benchmark = evaluation.evaluate_classification(test_target, prediction, dataset['Minority Class'])\n",
    "                        for key in additional_benchmark_fields:\n",
    "                            if key not in benchmark: # dont allow overwriting existing keys\n",
    "                                benchmark[key] = additional_benchmark_fields[key]\n",
    "                        classification_evaluation[(dataset_name,fold,preprocessing_method_name,classifier_name,k)] = benchmark\n",
    "                    # Remove unused data to let the garbage collector work\n",
    "                    del train_oversampled, train_target_oversampled\n",
    "                except Exception as e:\n",
    "                    print('Exception occured at', preprocessing_method_name)\n",
    "                    traceback.print_exc()\n",
    "    # Remove unused data to let the garbage collector work\n",
    "    del dataset['Data'], train, train_target, test, test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res = pd.DataFrame(classification_evaluation).transpose()\n",
    "res.index.names = (['Dataset','Fold', 'Oversampling', 'Classifier', 'k'])\n",
    "res.to_csv('results/{0}/results_detailed.csv'.format(session_id))\n",
    "grouped = res.groupby(level=['Dataset','Oversampling','Classifier','k'])\n",
    "res = grouped.describe()\n",
    "res.to_csv('results/{0}/results.csv'.format(session_id))\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pdf = PdfPages('results/{0}/results.pdf'.format(session_id))\n",
    "results_to_plot = res[['ROC AUC Score','Balanced F-Measure','F1-Score Macro', 'G-Measure', 'Diversity', 'Majority Accuracy', 'Minority Accuracy', 'FN', 'FP']]\n",
    "for dataset in res.index.levels[0].values:\n",
    "    fig,axes = plt.subplots(nrows=len(results_to_plot.columns), ncols=1, figsize=(5,len(results_to_plot.columns)))\n",
    "    fig.subplots_adjust(top=5, hspace=0.3)\n",
    "    for i, score in enumerate(results_to_plot.columns):\n",
    "#         ax=axes[i*2]\n",
    "#         ax.set_title(dataset + ' // ' + score)\n",
    "        unstacked_res = results_to_plot[score][dataset].unstack(level=[0,2])\n",
    "#         mean = unstacked_res.loc[pd.IndexSlice[:,'mean'],:]\n",
    "#         sd = unstacked_res.loc[pd.IndexSlice[:,'std'],:]\n",
    "#         mean.index = mean.index.droplevel(1)\n",
    "#         sd.index = sd.index.droplevel(1)\n",
    "        maximum = unstacked_res.loc[pd.IndexSlice[:,'max'],:].dropna(axis=1).values.max()\n",
    "        lim = (0,1) if maximum <= 1 else (0,maximum)\n",
    "# #         mean.plot(kind='barh',ax=ax, width=.8, xerr=sd, xlim=lim )\n",
    "#         ax.legend(bbox_to_anchor=(1.9, 1))\n",
    "#         ax.set_yticklabels([])\n",
    "#         ax.set_ylabel('')\n",
    "#         ax.set_yticks(np.arange(0,0.8*3))\n",
    "        ax=axes[i]\n",
    "        ax.set_title(dataset + ' // ' + score)\n",
    "        unstacked_res.plot(kind='box', ax=ax, xlim=lim, vert=False)\n",
    "#         ax.set_xticklabels([n.get_text()[1:].split(',')[0][0:15] for n in ax.get_xticklabels()])\n",
    "    pdf.savefig(fig, bbox_inches='tight')\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# res_detailed = pd.read_csv('./results/2017-04-21 22h55m59/results_detailed.csv')\n",
    "# res_detailed = res_detailed.loc[res_detailed['Oversampling'] == 'K-Means on Autoencoder Encoding']\n",
    "# ae_err = pd.Series({1:0.5863,2:0.5850,3:0.5852,4:0.5862,5:0.5890}, name='AE Error')\n",
    "# res_detailed = res_detailed.join(ae_err, on='Fold')\n",
    "\n",
    "# res_detailed_26_04 = pd.read_csv('./results/2017-04-26 12h12m03/results_detailed.csv')\n",
    "# res_detailed_26_04 = res_detailed_26_04.loc[res_detailed_26_04['Oversampling'] == 'K-Means on Autoencoder Encoding']\n",
    "# ae_err_26_04 = pd.Series({1:0.5864,2:0.5853,3:0.5846,4:0.5861,5:0.5883}, name='AE Error')\n",
    "# res_detailed_26_04 = res_detailed_26_04.join(ae_err_26_04, on='Fold')\n",
    "\n",
    "# res_detailed = pd.concat([res_detailed,res_detailed_26_04], axis=0)\n",
    "\n",
    "# display(res_detailed)\n",
    "\n",
    "# res_detailed = res_detailed.sort_values(by='AE Error') \n",
    "\n",
    "# for c in ['Balanced F-Measure','G-Measure','Majority Accuracy','Minority Accuracy','ROC AUC Score']:\n",
    "#     plt.plot(res_detailed['AE Error'], res_detailed[c], '-o', label=c)\n",
    "# plt.legend(loc=4)\n",
    "# plt.title('Relation to AE Error')\n",
    "\n",
    "# plt.figure(figsize=(3,2))\n",
    "# pd.Series(res_detailed.corr()['AE Error'], name='Correlation with AE Error').plot.barh(title='Correlation with AE Error')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
